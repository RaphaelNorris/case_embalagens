# ğŸ“¦ Case Embalagens - ADAMI Production Optimization

![Lifecycle do Machine Learning](lifecycle_ml.png)

[![CI](https://github.com/RaphaelNorris/case_embalagens/actions/workflows/ci.yml/badge.svg)](https://github.com/RaphaelNorris/case_embalagens/actions/workflows/ci.yml)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)

## ğŸ“‹ VisÃ£o Geral

Projeto de otimizaÃ§Ã£o de processos de produÃ§Ã£o para ADAMI (indÃºstria de embalagens) em parceria com AMCOM. Implementa um pipeline completo de Machine Learning seguindo metodologias CRISP-DM e CD4ML (Continuous Delivery for Machine Learning).

### Objetivos Principais

- **AnÃ¡lise de Paradas de MÃ¡quinas**: Identificar padrÃµes e causas de paradas nÃ£o programadas
- **OtimizaÃ§Ã£o de ProduÃ§Ã£o**: Prever tempo de produÃ§Ã£o e otimizar alocaÃ§Ã£o de recursos
- **GestÃ£o de Facas/LÃ¢minas**: Monitorar ciclo de vida e performance de ferramentas de corte
- **Analytics em Tempo Real**: Dashboard Streamlit para visualizaÃ§Ã£o de KPIs de produÃ§Ã£o

## ğŸ—ï¸ Arquitetura do Projeto

```
case_embalagens/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/          # CI/CD pipelines
â”œâ”€â”€ project_data_science/   # Projeto principal de Data Science
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ 01 - raw/      # Dados brutos (Bronze layer)
â”‚   â”‚   â”œâ”€â”€ 02 - trusted/  # Dados limpos (Silver layer)
â”‚   â”‚   â”œâ”€â”€ 03 - ml/       # Features para ML
â”‚   â”‚   â””â”€â”€ 04 - refined/  # Dados analÃ­ticos (Gold layer)
â”‚   â”œâ”€â”€ docs/              # DocumentaÃ§Ã£o do projeto
â”‚   â”œâ”€â”€ notebooks/         # Jupyter notebooks organizados
â”‚   â”‚   â”œâ”€â”€ eda/          # AnÃ¡lise exploratÃ³ria
â”‚   â”‚   â”‚   â”œâ”€â”€ initial/  # ExploraÃ§Ãµes iniciais
â”‚   â”‚   â”‚   â””â”€â”€ refined/  # AnÃ¡lises refinadas
â”‚   â”‚   â””â”€â”€ overview/     # Notebooks de visÃ£o geral
â”‚   â”œâ”€â”€ src/              # CÃ³digo fonte principal
â”‚   â”‚   â”œâ”€â”€ config.py     # ConfiguraÃ§Ã£o centralizada
â”‚   â”‚   â”œâ”€â”€ logger.py     # Logging estruturado
â”‚   â”‚   â”œâ”€â”€ data/         # MÃ³dulos de dados
â”‚   â”‚   â”‚   â”œâ”€â”€ conn_oracle.py
â”‚   â”‚   â”‚   â”œâ”€â”€ conn_sql.py
â”‚   â”‚   â”‚   â””â”€â”€ data_quality_analytics.py
â”‚   â”‚   â”œâ”€â”€ features/     # Feature engineering
â”‚   â”‚   â”‚   â””â”€â”€ build_features.py
â”‚   â”‚   â”œâ”€â”€ models/       # Modelos ML
â”‚   â”‚   â”‚   â”œâ”€â”€ train_model.py
â”‚   â”‚   â”‚   â””â”€â”€ predict_model.py
â”‚   â”‚   â””â”€â”€ app.py        # Streamlit dashboard
â”‚   â”œâ”€â”€ tests/            # Testes unitÃ¡rios
â”‚   â””â”€â”€ pyproject.toml    # ConfiguraÃ§Ã£o do projeto
â”œâ”€â”€ project_data_engineer/ # Pipeline de dados (Airflow)
â”‚   â””â”€â”€ dags/             # DAGs do Airflow
â”œâ”€â”€ Makefile              # Comandos de automaÃ§Ã£o
â”œâ”€â”€ .pre-commit-config.yaml
â””â”€â”€ .env.example          # Template de variÃ¡veis de ambiente
```

## ğŸš€ Quick Start

### PrÃ©-requisitos

- Python 3.10+
- Oracle Client (para conexÃµes Oracle)
- ODBC Driver for SQL Server

### InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/RaphaelNorris/case_embalagens.git
cd case_embalagens

# Configure variÃ¡veis de ambiente
cp .env.example .env
# Edite .env com suas credenciais

# Instale dependÃªncias de desenvolvimento
make install-dev

# Configure pre-commit hooks
pre-commit install
```

### Uso RÃ¡pido

```bash
# Executar testes
make test

# Executar com coverage
make test-cov

# Formatar cÃ³digo
make format

# Executar linting
make lint

# Executar app Streamlit (anÃ¡lise de facas)
make app-facas

# Limpar arquivos temporÃ¡rios
make clean
```

## ğŸ“Š Camadas de Dados (Medallion Architecture)

### ğŸ¥‰ Bronze Layer (01 - raw)
Dados brutos extraÃ­dos diretamente das fontes sem transformaÃ§Ãµes.

### ğŸ¥ˆ Silver Layer (02 - trusted)
Dados limpos, padronizados e validados. Principais tabelas:
- `tb_clientes.parquet`: InformaÃ§Ãµes de clientes
- `tb_pedidos.parquet`: Ordens de produÃ§Ã£o
- `tb_itens.parquet`: Itens dos pedidos
- `tb_maquinas.parquet`: Dados das mÃ¡quinas
- `tb_facas.parquet`: InformaÃ§Ãµes de facas/lÃ¢minas
- `tb_paradas.parquet`: Eventos de parada de mÃ¡quinas
- `tb_tarefcon.parquet`: Controle de tarefas de produÃ§Ã£o

### ğŸ¥‡ Gold Layer (04 - refined)
Dados agregados e prontos para anÃ¡lise/BI.

### ğŸ¤– ML Layer (03 - ml)
Features engineeradas prontas para treinamento de modelos.

## ğŸ”§ Principais Funcionalidades

### 1. ConexÃµes de Banco de Dados

```python
from src.data.conn_oracle import oracle_connection
from src.data.conn_sql import sqlserver_connection

# Oracle (com context manager)
with oracle_connection('trusted') as conn:
    df = pd.read_sql("SELECT * FROM tb_pedidos", conn)

# SQL Server
with sqlserver_connection() as conn:
    df = pd.read_sql("SELECT * FROM dbo.Clientes", conn)
```

### 2. Feature Engineering

```python
from src.features.build_features import (
    create_temporal_features,
    create_production_features,
    create_stoppage_features
)

# Criar features temporais
df = create_temporal_features(df, datetime_col='data_producao')

# Features de produÃ§Ã£o
df = create_production_features(df, group_cols=['cod_maquina'])

# Features de paradas
df_paradas = create_stoppage_features(df_paradas, df_tarefcon)
```

### 3. Treinamento de Modelos

```python
from src.models.train_model import train_production_model, save_model

# Treinar modelo
model, metrics = train_production_model(
    X, y,
    model_type='random_forest',
    test_size=0.2
)

# Salvar modelo
save_model(model, 'production_optimizer_v1', metadata=metrics)
```

### 4. Dashboard Streamlit

```bash
cd project_data_science/src
streamlit run app.py
```

## ğŸ§ª Testes

```bash
# Executar todos os testes
pytest

# Com coverage
pytest --cov=src --cov-report=html

# Testes especÃ­ficos
pytest tests/test_features.py -v
```

## ğŸ“ Estrutura de Notebooks

Os notebooks seguem a convenÃ§Ã£o de nomenclatura:

```
##.#-iniciais-descriÃ§Ã£o-data.ipynb
```

Exemplo: `01.0-rn-exploratory-analysis-20240115.ipynb`

### Categorias de Notebooks

- **00-09**: AnÃ¡lise exploratÃ³ria individual de tabelas
- **10-19**: AnÃ¡lises cruzadas e relacionamentos
- **20-29**: Feature engineering
- **30-39**: Modelagem e experimentaÃ§Ã£o
- **40-49**: ProduÃ§Ã£o e deploy

## ğŸ” Qualidade de CÃ³digo

O projeto utiliza vÃ¡rias ferramentas para garantir qualidade:

- **Ruff**: Linting e formataÃ§Ã£o rÃ¡pida (substitui Black, isort, flake8)
- **MyPy**: Type checking estÃ¡tico
- **Pytest**: Framework de testes
- **Pre-commit**: Hooks automÃ¡ticos antes de commits

## ğŸ“š DocumentaÃ§Ã£o

DocumentaÃ§Ã£o completa disponÃ­vel em `project_data_science/docs/`:

- **data_source.md**: DescriÃ§Ã£o das fontes de dados
- **data_structure.md**: Estrutura das tabelas
- **pipelines.md**: Arquitetura dos pipelines
- **data_quality/**: RelatÃ³rios de qualidade de dados

## ğŸ” SeguranÃ§a

- âœ… Credenciais gerenciadas via variÃ¡veis de ambiente (.env)
- âœ… `.gitignore` configurado para nÃ£o commitar dados sensÃ­veis
- âœ… Pre-commit hook para detecÃ§Ã£o de secrets
- âœ… ValidaÃ§Ã£o de dados com Pydantic

## ğŸ› ï¸ Stack TecnolÃ³gica

### Data & ML
- **Pandas, NumPy**: ManipulaÃ§Ã£o de dados
- **Scikit-learn, XGBoost, LightGBM**: Machine Learning
- **Streamlit**: Dashboards interativos

### Databases
- **Oracle DB**: Banco de dados principal
- **SQL Server**: Analytics e BI

### DevOps & Tools
- **Airflow**: OrquestraÃ§Ã£o de pipelines
- **GitHub Actions**: CI/CD
- **Pre-commit**: Hooks de qualidade
- **Ruff**: Linting/formataÃ§Ã£o
- **Pytest**: Testing

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### ConvenÃ§Ãµes de CÃ³digo

- Seguir PEP 8 (automaticamente via Ruff)
- Type hints em todas as funÃ§Ãµes pÃºblicas
- Docstrings no formato Google
- Testes para novas funcionalidades

## ğŸ“„ LicenÃ§a

Este projeto Ã© propriedade de ADAMI em parceria com AMCOM.

## ğŸ‘¥ Autores

- **Raphael Norris** - *Data Science Lead*

## ğŸ™ Agradecimentos

- ADAMI - Por fornecer os dados e expertise de domÃ­nio
- AMCOM - Parceria tecnolÃ³gica

---

**Desenvolvido com â¤ï¸ para otimizaÃ§Ã£o de processos industriais**
