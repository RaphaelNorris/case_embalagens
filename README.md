# ğŸ“¦ Case Embalagens - ADAMI Production Optimization

![Lifecycle do Machine Learning](docs/images/lifecycle_ml.png)

[![CI](https://github.com/RaphaelNorris/case_embalagens/actions/workflows/ci.yml/badge.svg)](https://github.com/RaphaelNorris/case_embalagens/actions/workflows/ci.yml)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)

> **ğŸ¯ Status**: RefatoraÃ§Ã£o completa concluÃ­da (v2.0.0) - Estrutura modular, documentaÃ§Ã£o completa e boas prÃ¡ticas de engenharia de software para ciÃªncia de dados.

## ğŸ“‹ VisÃ£o Geral

Projeto de otimizaÃ§Ã£o de processos de produÃ§Ã£o para ADAMI (indÃºstria de embalagens) em parceria com AMCOM. Implementa um pipeline completo de Machine Learning seguindo metodologias CRISP-DM e CD4ML (Continuous Delivery for Machine Learning).

### Objetivos Principais

- **AnÃ¡lise de Paradas de MÃ¡quinas**: Identificar padrÃµes e causas de paradas nÃ£o programadas
- **OtimizaÃ§Ã£o de ProduÃ§Ã£o**: Prever tempo de produÃ§Ã£o e otimizar alocaÃ§Ã£o de recursos
- **GestÃ£o de Facas/LÃ¢minas**: Monitorar ciclo de vida e performance de ferramentas de corte
- **Analytics em Tempo Real**: Dashboard Streamlit para visualizaÃ§Ã£o de KPIs de produÃ§Ã£o

## ğŸ—ï¸ Arquitetura do Projeto

```
case_embalagens/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/              # CI/CD pipelines
â”œâ”€â”€ docs/                       # DocumentaÃ§Ã£o e assets
â”‚   â””â”€â”€ images/                 # Imagens e diagramas
â”œâ”€â”€ project_data_science/       # ğŸ”¬ Projeto principal de Data Science
â”‚   â”œâ”€â”€ data/                   # Camadas de dados (Medallion Architecture)
â”‚   â”‚   â”œâ”€â”€ 01 - raw/          # ğŸ¥‰ Bronze: Dados brutos
â”‚   â”‚   â”œâ”€â”€ 02 - trusted/      # ğŸ¥ˆ Silver: Dados limpos
â”‚   â”‚   â”œâ”€â”€ 03 - ml/           # ğŸ¤– Features para ML
â”‚   â”‚   â””â”€â”€ 04 - refined/      # ğŸ¥‡ Gold: Dados analÃ­ticos
â”‚   â”œâ”€â”€ docs/                   # DocumentaÃ§Ã£o tÃ©cnica
â”‚   â”œâ”€â”€ models/                 # Modelos ML salvos
â”‚   â”œâ”€â”€ notebooks/              # ğŸ““ Jupyter notebooks organizados
â”‚   â”‚   â”œâ”€â”€ 01-eda-tables/     # EDA de tabelas individuais (9 notebooks)
â”‚   â”‚   â”œâ”€â”€ 02-eda-cross/      # AnÃ¡lises cruzadas (3 notebooks)
â”‚   â”‚   â”œâ”€â”€ 03-preprocessing/  # PrÃ©-processamento (2 notebooks)
â”‚   â”‚   â””â”€â”€ 04-production/     # Notebooks de produÃ§Ã£o (2 notebooks)
â”‚   â”œâ”€â”€ src/                    # ğŸ’» CÃ³digo fonte modular
â”‚   â”‚   â”œâ”€â”€ analysis/          # Processamento e anÃ¡lise
â”‚   â”‚   â”œâ”€â”€ dashboards/        # ğŸ“Š Dashboards Streamlit
â”‚   â”‚   â”‚   â”œâ”€â”€ components/    # Componentes UI reutilizÃ¡veis
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard_main.py
â”‚   â”‚   â”‚   â””â”€â”€ dashboard_facas.py
â”‚   â”‚   â”œâ”€â”€ data/              # ConexÃµes e qualidade
â”‚   â”‚   â”‚   â”œâ”€â”€ conn_oracle.py
â”‚   â”‚   â”‚   â”œâ”€â”€ conn_sql.py
â”‚   â”‚   â”‚   â””â”€â”€ data_quality.py
â”‚   â”‚   â”œâ”€â”€ features/          # Feature engineering
â”‚   â”‚   â”œâ”€â”€ models/            # ML (train/predict)
â”‚   â”‚   â”œâ”€â”€ viz/               # ğŸ“ˆ VisualizaÃ§Ãµes (Plotly)
â”‚   â”‚   â”œâ”€â”€ config.py          # ConfiguraÃ§Ãµes (Pydantic)
â”‚   â”‚   â””â”€â”€ logger.py          # Logging (Loguru)
â”‚   â”œâ”€â”€ scripts/                # Scripts utilitÃ¡rios
â”‚   â”œâ”€â”€ tests/                  # ğŸ§ª Testes unitÃ¡rios
â”‚   â””â”€â”€ pyproject.toml          # ConfiguraÃ§Ã£o do projeto
â”œâ”€â”€ project_data_engineer/      # âš™ï¸ Pipeline de dados (Airflow)
â”‚   â””â”€â”€ dags/                   # 43 DAGs de ETL
â”‚       â””â”€â”€ sql/                # Queries SQL (raw/trusted/refined)
â”œâ”€â”€ .env.example                # Template de variÃ¡veis
â”œâ”€â”€ .pre-commit-config.yaml     # Hooks de qualidade
â”œâ”€â”€ CHANGELOG.md                # HistÃ³rico de mudanÃ§as
â”œâ”€â”€ Makefile                    # Comandos de automaÃ§Ã£o
â”œâ”€â”€ PROJECT_STRUCTURE.md        # Estrutura detalhada
â””â”€â”€ README.md                   # Este arquivo
```

ğŸ“– **DocumentaÃ§Ã£o completa**: Cada mÃ³dulo possui seu prÃ³prio README com exemplos e guias de uso.

## ğŸš€ Quick Start

### PrÃ©-requisitos

- Python 3.10+
- Oracle Client (para conexÃµes Oracle)
- ODBC Driver for SQL Server

### InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/RaphaelNorris/case_embalagens.git
cd case_embalagens

# Configure variÃ¡veis de ambiente
cp .env.example .env
# Edite .env com suas credenciais

# Instale dependÃªncias de desenvolvimento
make install-dev

# Configure pre-commit hooks
pre-commit install
```

### Uso RÃ¡pido

```bash
# Executar testes
make test

# Executar com coverage
make test-cov

# Formatar cÃ³digo
make format

# Executar linting
make lint

# Executar app Streamlit (anÃ¡lise de facas)
make app-facas

# Limpar arquivos temporÃ¡rios
make clean
```

## ğŸ“Š Camadas de Dados (Medallion Architecture)

### ğŸ¥‰ Bronze Layer (01 - raw)
Dados brutos extraÃ­dos diretamente das fontes sem transformaÃ§Ãµes.

### ğŸ¥ˆ Silver Layer (02 - trusted)
Dados limpos, padronizados e validados. Principais tabelas:
- `tb_clientes.parquet`: InformaÃ§Ãµes de clientes
- `tb_pedidos.parquet`: Ordens de produÃ§Ã£o
- `tb_itens.parquet`: Itens dos pedidos
- `tb_maquinas.parquet`: Dados das mÃ¡quinas
- `tb_facas.parquet`: InformaÃ§Ãµes de facas/lÃ¢minas
- `tb_paradas.parquet`: Eventos de parada de mÃ¡quinas
- `tb_tarefcon.parquet`: Controle de tarefas de produÃ§Ã£o

### ğŸ¥‡ Gold Layer (04 - refined)
Dados agregados e prontos para anÃ¡lise/BI.

### ğŸ¤– ML Layer (03 - ml)
Features engineeradas prontas para treinamento de modelos.

## ğŸ”§ Principais Funcionalidades

### 1. ConexÃµes de Banco de Dados

```python
from src.data.conn_oracle import oracle_connection
from src.data.conn_sql import sqlserver_connection

# Oracle (com context manager)
with oracle_connection('trusted') as conn:
    df = pd.read_sql("SELECT * FROM tb_pedidos", conn)

# SQL Server
with sqlserver_connection() as conn:
    df = pd.read_sql("SELECT * FROM dbo.Clientes", conn)
```

### 2. Feature Engineering

```python
from src.features.build_features import (
    create_temporal_features,
    create_production_features,
    create_stoppage_features
)

# Criar features temporais
df = create_temporal_features(df, datetime_col='data_producao')

# Features de produÃ§Ã£o
df = create_production_features(df, group_cols=['cod_maquina'])

# Features de paradas
df_paradas = create_stoppage_features(df_paradas, df_tarefcon)
```

### 3. Treinamento de Modelos

```python
from src.models.train_model import train_production_model, save_model

# Treinar modelo
model, metrics = train_production_model(
    X, y,
    model_type='random_forest',
    test_size=0.2
)

# Salvar modelo
save_model(model, 'production_optimizer_v1', metadata=metrics)
```

### 4. Dashboards Streamlit

```bash
# Dashboard principal (multi-pÃ¡gina)
streamlit run project_data_science/src/dashboards/dashboard_main.py

# Dashboard de facas/lÃ¢minas
streamlit run project_data_science/src/dashboards/dashboard_facas.py

# Ou via Makefile
make app-main    # Dashboard principal
make app-facas   # Dashboard de facas
```

## ğŸ§ª Testes

```bash
# Executar todos os testes
pytest

# Com coverage
pytest --cov=src --cov-report=html

# Testes especÃ­ficos
pytest tests/test_features.py -v
```

## ğŸ“ Estrutura de Notebooks

Os notebooks seguem a convenÃ§Ã£o de nomenclatura:

```
##.#-author-description-YYYYMMDD.ipynb
```

Exemplo: `01.0-rn-eda-clientes-20240101.ipynb`

### Categorias de Notebooks

- **ğŸ“Š 01-eda-tables/** (9 notebooks): AnÃ¡lise exploratÃ³ria de tabelas individuais
  - Clientes, Pedidos, Itens, MÃ¡quinas, Facas, Paradas, Tarefcon
  - Metodologia: Load â†’ Describe â†’ Quality â†’ Visualize â†’ Insights

- **ğŸ”— 02-eda-cross/** (3 notebooks): AnÃ¡lises cruzadas entre tabelas
  - Relacionamentos pedidos-itens, tarefcon-paradas, tarefcon-itens
  - ValidaÃ§Ã£o de integridade referencial

- **ğŸ§¹ 03-preprocessing/** (2 notebooks): PrÃ©-processamento e limpeza
  - Pipeline de transformaÃ§Ã£o Raw â†’ Trusted â†’ Refined
  - Tratamento de outliers, missing values, tipos

- **ğŸš€ 04-production/** (2 notebooks): Notebooks prontos para produÃ§Ã£o
  - Overview piloto e associaÃ§Ã£o temporal
  - IntegraÃ§Ã£o com Airflow

ğŸ“š **Cada categoria possui seu prÃ³prio README** com documentaÃ§Ã£o detalhada.

## ğŸ” Qualidade de CÃ³digo

O projeto utiliza vÃ¡rias ferramentas para garantir qualidade:

- **Ruff**: Linting e formataÃ§Ã£o rÃ¡pida (substitui Black, isort, flake8)
- **MyPy**: Type checking estÃ¡tico
- **Pytest**: Framework de testes
- **Pre-commit**: Hooks automÃ¡ticos antes de commits

## ğŸ“š DocumentaÃ§Ã£o

### DocumentaÃ§Ã£o por MÃ³dulo

Cada mÃ³dulo possui documentaÃ§Ã£o detalhada com exemplos prÃ¡ticos:

- **[src/README.md](project_data_science/src/README.md)**: VisÃ£o geral de todos os mÃ³dulos
- **[src/data/README.md](project_data_science/src/data/README.md)**: ConexÃµes e qualidade de dados
- **[src/features/README.md](project_data_science/src/features/README.md)**: Feature engineering
- **[src/models/README.md](project_data_science/src/models/README.md)**: Treinamento e prediÃ§Ã£o
- **[src/viz/README.md](project_data_science/src/viz/README.md)**: VisualizaÃ§Ãµes Plotly
- **[src/analysis/README.md](project_data_science/src/analysis/README.md)**: Processamento de dados
- **[src/dashboards/README.md](project_data_science/src/dashboards/README.md)**: Dashboards Streamlit

### DocumentaÃ§Ã£o de Dados

- **[data/README.md](project_data_science/data/README.md)**: Medallion Architecture (Bronze/Silver/Gold)
- **[data/01 - raw/README.md](project_data_science/data/01%20-%20raw/README.md)**: Camada Bronze
- **[data/02 - trusted/README.md](project_data_science/data/02%20-%20trusted/README.md)**: Camada Silver
- **[data/03 - ml/README.md](project_data_science/data/03%20-%20ml/README.md)**: Features ML
- **[data/04 - refined/README.md](project_data_science/data/04%20-%20refined/README.md)**: Camada Gold

### DocumentaÃ§Ã£o de Notebooks

- **[notebooks/01-eda-tables/README.md](project_data_science/notebooks/01-eda-tables/README.md)**: EDA de tabelas
- **[notebooks/02-eda-cross/README.md](project_data_science/notebooks/02-eda-cross/README.md)**: AnÃ¡lises cruzadas
- **[notebooks/03-preprocessing/README.md](project_data_science/notebooks/03-preprocessing/README.md)**: PrÃ©-processamento
- **[notebooks/04-production/README.md](project_data_science/notebooks/04-production/README.md)**: ProduÃ§Ã£o

### DocumentaÃ§Ã£o TÃ©cnica

- **[PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md)**: Estrutura completa do projeto
- **[CHANGELOG.md](CHANGELOG.md)**: HistÃ³rico de mudanÃ§as
- **docs/**: Fontes de dados, estrutura, pipelines, relatÃ³rios de qualidade

## ğŸ” SeguranÃ§a

- âœ… Credenciais gerenciadas via variÃ¡veis de ambiente (.env)
- âœ… `.gitignore` configurado para nÃ£o commitar dados sensÃ­veis
- âœ… Pre-commit hook para detecÃ§Ã£o de secrets
- âœ… ValidaÃ§Ã£o de dados com Pydantic

## ğŸ› ï¸ Stack TecnolÃ³gica

### Data & ML
- **Pandas, NumPy**: ManipulaÃ§Ã£o de dados
- **Scikit-learn, XGBoost, LightGBM**: Machine Learning
- **Streamlit**: Dashboards interativos

### Databases
- **Oracle DB**: Banco de dados principal
- **SQL Server**: Analytics e BI

### DevOps & Tools
- **Airflow**: OrquestraÃ§Ã£o de pipelines
- **GitHub Actions**: CI/CD
- **Pre-commit**: Hooks de qualidade
- **Ruff**: Linting/formataÃ§Ã£o
- **Pytest**: Testing

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### ConvenÃ§Ãµes de CÃ³digo

- Seguir PEP 8 (automaticamente via Ruff)
- Type hints em todas as funÃ§Ãµes pÃºblicas
- Docstrings no formato Google
- Testes para novas funcionalidades

## ğŸ“„ LicenÃ§a

Este projeto Ã© propriedade de ADAMI em parceria com AMCOM.

## ğŸ‘¥ Autores

- **Raphael Norris** - *Data Science Lead*

## ğŸ™ Agradecimentos

- ADAMI - Por fornecer os dados e expertise de domÃ­nio
- AMCOM - Parceria tecnolÃ³gica

---

**Desenvolvido com â¤ï¸ para otimizaÃ§Ã£o de processos industriais**
