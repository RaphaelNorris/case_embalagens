# -*- coding: utf-8 -*-
"""
DAG: medidas_papel_spark
- Cria a tabela RAW_MEDIDAS_PAPEL no Oracle (Common SQL)
- Executa um job Spark que insere 5 linhas via JDBC

PRÉ-REQUISITOS (no venv do Airflow):
  pip install -U apache-airflow-providers-common-sql apache-airflow-providers-apache-spark apache-airflow-providers-oracle oracledb

CONNECTIONS NA UI:
  1) oracle_raw (Conn Type: Oracle)
     Host: ORACLE10G
     Port: 1521
     Login: TRIMBOX_RAW
     Password: <sua_senha>
     Extra (JSON): {"service_name": "adami.adami.corporativo"}

  2) spark_local (Conn Type: Spark)
     Extra (JSON): {"master": "local[*]", "deploy_mode": "client", "spark_binary": "spark-submit"}
     # Se 'spark-submit' não estiver no PATH, use caminho completo em spark_binary.

OBS:
  - O Spark pegará o driver JDBC do Maven (packages). Sem internet, troque 'packages' por 'jars' apontando para um ojdbc8.jar local.
"""

from datetime import datetime
from airflow import DAG
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

ORACLE_CONN_ID = "oracle_raw"
SPARK_CONN_ID = "spark_local"

CREATE_TABLE_SQL = """
DECLARE
  e_exists EXCEPTION;
  PRAGMA EXCEPTION_INIT(e_exists, -955);
BEGIN
  EXECUTE IMMEDIATE q'[
    CREATE TABLE RAW_MEDIDAS_PAPEL (
      ID                 NUMBER GENERATED BY DEFAULT ON NULL AS IDENTITY,
      TIPO               VARCHAR2(30)  NOT NULL,
      NOME               VARCHAR2(100) NOT NULL,
      LARGURA_MM         NUMBER(10,2),
      COMPRIMENTO_MM     NUMBER(10,2),
      ESPESSURA_MICRONS  NUMBER(10,2),
      DATA_CRIACAO       DATE DEFAULT SYSDATE,
      CONSTRAINT PK_RAW_MEDIDAS_PAPEL PRIMARY KEY (ID)
    )
  ]';
EXCEPTION WHEN e_exists THEN NULL;
END;
"""

with DAG(
    dag_id="medidas_papel_spark",
    start_date=datetime(2025, 1, 1),
    schedule=None,         # rode manualmente
    catchup=False,
    tags=["oracle", "spark", "raw"],
    description="Cria RAW_MEDIDAS_PAPEL e ingere 5 linhas via Spark (JDBC)",
) as dag:

    # 1) Cria a tabela no Oracle (idempotente)
    create_table = SQLExecuteQueryOperator(
        task_id="create_table_raw_medidas_papel",
        conn_id=ORACLE_CONN_ID,
        sql=CREATE_TABLE_SQL,
    )

    # 2) Roda o job Spark que escreve 5 linhas via JDBC
    #    -> master/deploy_mode/spark_binary vêm da Connection spark_local (não passe 'master' aqui!)
    spark_ingest = SparkSubmitOperator(
        task_id="spark_ingest_raw_medidas_papel",
        application="/home/adami/airflow/dags/spark-apps/medidas_papel_ingest.py",
        conn_id=SPARK_CONN_ID,
        # Baixa automaticamente o driver JDBC do Maven Central:
        packages="com.oracle.database.jdbc:ojdbc8:23.4.0.24.05",
        # Se estiver sem internet, comente 'packages' e use um JAR local:
        # jars="/opt/spark/jars/ojdbc8.jar",
        env_vars={
            # Monta a URL JDBC a partir da Connection oracle_raw
            "ORACLE_JDBC_URL": "jdbc:oracle:thin:@//{{ conn.oracle_raw.host }}:{{ conn.oracle_raw.port }}/{{ conn.oracle_raw.extra_dejson.service_name }}",
            "ORACLE_USER": "{{ conn.oracle_raw.login }}",
            "ORACLE_PASSWORD": "{{ conn.oracle_raw.password }}",
            "ORACLE_TABLE": "RAW_MEDIDAS_PAPEL",
        },
        # Exemplos de configs extras (opcional):
        # conf={"spark.executor.memory": "1g", "spark.driver.memory": "1g"},
        verbose=False,
    )

    create_table >> spark_ingest
