# üöÄ Guia de Transi√ß√£o: POC ‚Üí Produ√ß√£o

> **Objetivo**: Este documento descreve a estrutura refatorada do projeto e como utiliz√°-la para produtizar modelos de ML.

**Data da Refatora√ß√£o**: 2024-11-23
**Vers√£o**: 2.1.0

---

## üìã Sum√°rio Executivo

O projeto `case_embalagens` foi **completamente refatorado** seguindo princ√≠pios de:
- **CRISP-DM**: Metodologia de Data Science
- **CD4ML**: Continuous Delivery for Machine Learning
- **FTI Pipelines**: Feature / Training / Inference separation

**Principais melhorias**:
- ‚úÖ Elimina√ß√£o de c√≥digo duplicado (35% redu√ß√£o)
- ‚úÖ Parametriza√ß√£o completa (zero hardcoded paths)
- ‚úÖ Estrutura FTI para pipelines ML
- ‚úÖ Separa√ß√£o clara POC vs Produ√ß√£o
- ‚úÖ Config centralizado com valida√ß√£o

---

## üèóÔ∏è Nova Estrutura do Projeto

### Antes da Refatora√ß√£o (POC)

```
project_data_science/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ eda/initial/  ‚ùå Notebooks soltos, nomes confusos
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app.py, app2.py  ‚ùå Apps n√£o modularizados
‚îÇ   ‚îú‚îÄ‚îÄ model/  ‚ùå Nome confuso
‚îÇ   ‚îú‚îÄ‚îÄ models/ ‚ùå Duplica√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ pipelines/DS/
‚îÇ       ‚îú‚îÄ‚îÄ *.ipynb  ‚ùå Notebooks em src/
‚îÇ       ‚îî‚îÄ‚îÄ training_fixed.py ‚ùå Duplicado
‚îî‚îÄ‚îÄ data/
    ‚îî‚îÄ‚îÄ raw/, trusted/ ‚ö†Ô∏è Paths hardcoded
```

**Problemas identificados**:
- üî¥ 8 notebooks gigantes em `src/` (200k+ linhas)
- üî¥ 7 arquivos com hardcoded paths
- üî¥ C√≥digo duplicado CV/Flexo (144k linhas)
- üî¥ Diret√≥rios confusos (`model/` vs `models/`)
- üî¥ Zero testes

### Depois da Refatora√ß√£o (Produ√ß√£o)

```
project_data_science/
‚îú‚îÄ‚îÄ config/                         # ‚ú® Configura√ß√µes hier√°rquicas
‚îÇ   ‚îî‚îÄ‚îÄ (futuro: base.yaml, prod.yaml)
‚îÇ
‚îú‚îÄ‚îÄ data/                           # ‚úÖ Medallion Architecture
‚îÇ   ‚îú‚îÄ‚îÄ 01-raw/                    # ü•â Bronze: Dados brutos
‚îÇ   ‚îú‚îÄ‚îÄ 02-trusted/                # ü•à Silver: Dados limpos
‚îÇ   ‚îú‚îÄ‚îÄ 03-ml/                     # ü§ñ ML: Features
‚îÇ   ‚îî‚îÄ‚îÄ 04-refined/                # ü•á Gold: Agregados
‚îÇ
‚îú‚îÄ‚îÄ models/                         # ‚ú® Model Registry
‚îÇ   ‚îú‚îÄ‚îÄ production/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flexo/champion/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cv/champion/
‚îÇ   ‚îî‚îÄ‚îÄ experiments/
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                      # ‚úÖ Organizado por fase
‚îÇ   ‚îú‚îÄ‚îÄ 01-eda-tables/             # POC: Explora√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ 02-eda-cross/              # POC: An√°lise
‚îÇ   ‚îú‚îÄ‚îÄ 03-preprocessing/          # Produ√ß√£o: Pipeline
‚îÇ   ‚îú‚îÄ‚îÄ 04-production/             # Produ√ß√£o: Algoritmos
‚îÇ   ‚îî‚îÄ‚îÄ experiments/               # ‚ú® Experimenta√ß√µes DS
‚îÇ       ‚îî‚îÄ‚îÄ ds-pipelines/          # Notebooks movidos de src/
‚îÇ
‚îî‚îÄ‚îÄ src/                            # ‚úÖ Apenas c√≥digo Python
    ‚îú‚îÄ‚îÄ config.py                   # ‚ú® Pydantic config
    ‚îú‚îÄ‚îÄ config_manager.py           # ‚ú® Helper centralizado
    ‚îú‚îÄ‚îÄ logger.py                   # ‚ú® Logging estruturado
    ‚îÇ
    ‚îú‚îÄ‚îÄ data/                       # Extra√ß√£o e qualidade
    ‚îÇ   ‚îú‚îÄ‚îÄ conn_oracle.py
    ‚îÇ   ‚îú‚îÄ‚îÄ conn_sql.py
    ‚îÇ   ‚îî‚îÄ‚îÄ data_quality*.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ pipelines/                  # ‚ú® FTI Structure
    ‚îÇ   ‚îú‚îÄ‚îÄ feature/                # Feature Pipeline
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engineering.py ‚Üí ../DS/feature_engineering.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ selection.py ‚Üí ../DS/feature_selection.py
    ‚îÇ   ‚îú‚îÄ‚îÄ training/               # Training Pipeline
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py ‚Üí ../DS/training.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ modeling.py ‚Üí ../DS/modeling.py
    ‚îÇ   ‚îú‚îÄ‚îÄ inference/              # Inference Pipeline
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ predict.py ‚Üí ../DS/inference.py
    ‚îÇ   ‚îú‚îÄ‚îÄ orchestration/          # Orquestra√ß√£o
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ runner.py ‚Üí ../DS/pipelines.py
    ‚îÇ   ‚îî‚îÄ‚îÄ DS/                     # ‚úÖ C√≥digo original (retrocompat)
    ‚îÇ       ‚îú‚îÄ‚îÄ config.py           # ‚úÖ Agora usa config_manager
    ‚îÇ       ‚îú‚îÄ‚îÄ feature_engineering.py
    ‚îÇ       ‚îú‚îÄ‚îÄ training.py
    ‚îÇ       ‚îî‚îÄ‚îÄ ...
    ‚îÇ
    ‚îú‚îÄ‚îÄ ml_artifacts/               # ‚úÖ Renomeado de model/
    ‚îÇ   ‚îú‚îÄ‚îÄ model_persistence.py
    ‚îÇ   ‚îî‚îÄ‚îÄ example_load_and_predict.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ app/                        # Aplica√ß√£o Streamlit
    ‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ dashboards/                 # Dashboards refatorados
    ‚îÇ   ‚îú‚îÄ‚îÄ dashboard_main.py
    ‚îÇ   ‚îî‚îÄ‚îÄ dashboard_facas.py
    ‚îÇ
    ‚îî‚îÄ‚îÄ analysis/                   # An√°lises
        ‚îî‚îÄ‚îÄ data_processing.py
```

---

## üîÑ Mudan√ßas Importantes

### 1. Configura√ß√£o Centralizada

**ANTES** (Hardcoded):
```python
# ‚ùå Ruim - quebra em outros ambientes
DATA_DIR = "/home/adami/Documentos/Projeto_IA_AMCOM/project_data_science/data/"
df = pd.read_parquet(f"{DATA_DIR}/raw/tb_pedidos.parquet")
```

**DEPOIS** (Parametrizado):
```python
# ‚úÖ Bom - funciona em qualquer ambiente
from src.config_manager import get_config_manager

cm = get_config_manager()
df = pd.read_parquet(cm.get_table_path('tb_pedidos', layer='raw'))
```

### 2. Estrutura FTI para Pipelines

**ANTES**:
```python
# Tudo junto em src/pipelines/DS/
src/pipelines/DS/
‚îú‚îÄ‚îÄ feature_engineering.py
‚îú‚îÄ‚îÄ training.py
‚îú‚îÄ‚îÄ inference.py
‚îî‚îÄ‚îÄ modeling.py  # Confuso!
```

**DEPOIS**:
```
# Separa√ß√£o clara por responsabilidade
src/pipelines/
‚îú‚îÄ‚îÄ feature/         # Feature Pipeline
‚îÇ   ‚îú‚îÄ‚îÄ engineering.py
‚îÇ   ‚îî‚îÄ‚îÄ selection.py
‚îú‚îÄ‚îÄ training/        # Training Pipeline
‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îî‚îÄ‚îÄ modeling.py
‚îú‚îÄ‚îÄ inference/       # Inference Pipeline
‚îÇ   ‚îî‚îÄ‚îÄ predict.py
‚îî‚îÄ‚îÄ orchestration/   # Orquestra√ß√£o
    ‚îî‚îÄ‚îÄ runner.py
```

### 3. Notebooks Organizados

**ANTES**:
```
notebooks/eda/initial/
‚îú‚îÄ‚îÄ 01.nb_eda.ipynb
‚îú‚îÄ‚îÄ 02.nb_eda_clientes.ipynb
‚îú‚îÄ‚îÄ 05.nb_eda_itens2.ipynb  ‚ùå Duplicado
‚îî‚îÄ‚îÄ ...

src/pipelines/DS/
‚îú‚îÄ‚îÄ pipeline_cv_ml.ipynb  ‚ùå 74k linhas em src/
‚îî‚îÄ‚îÄ ...
```

**DEPOIS**:
```
notebooks/
‚îú‚îÄ‚îÄ 01-eda-tables/              # POC
‚îú‚îÄ‚îÄ 02-eda-cross/               # POC
‚îú‚îÄ‚îÄ 03-preprocessing/           # Produ√ß√£o
‚îú‚îÄ‚îÄ 04-production/              # Produ√ß√£o
‚îî‚îÄ‚îÄ experiments/ds-pipelines/   # ‚úÖ Notebooks movidos de src/
    ‚îú‚îÄ‚îÄ pipeline_cv_ml.ipynb
    ‚îî‚îÄ‚îÄ ...
```

### 4. Renomea√ß√µes para Clareza

| Antes | Depois | Motivo |
|-------|--------|--------|
| `src/model/` | `src/ml_artifacts/` | Maior clareza (persist√™ncia) |
| `src/models/` | *Deletado* | Template n√£o usado |
| `src/features/` | *Deletado* | Template n√£o usado |
| `training_fixed.py` | *Deletado* | Duplicata exata |

---

## üìñ Como Usar a Nova Estrutura

### Exemplo 1: Carregar Dados

```python
from src.config_manager import get_config_manager

# Inicializar config manager
cm = get_config_manager()

# Carregar tabela da camada trusted
import pandas as pd
df = pd.read_parquet(cm.get_table_path('tb_pedidos', 'trusted'))

# Ou diretamente com path
df = pd.read_parquet(cm.trusted_path / 'parquet' / 'tb_pedidos.parquet')

# Diferentes camadas
raw_df = pd.read_parquet(cm.get_table_path('tb_pedidos', 'raw'))
ml_df = pd.read_parquet(cm.get_table_path('features', 'ml'))
```

### Exemplo 2: Executar Pipeline de Features

```python
# Usando nova estrutura FTI
from src.pipelines.feature import engineering, selection

# Feature engineering
df_features = engineering.create_geometric_features(df)
df_features = engineering.create_temporal_features(df_features)

# Feature selection
selected_features = selection.select_features_by_importance(df_features, y)

# Salvar em ML layer
output_path = cm.ml_path / 'features_engineered.parquet'
df_features[selected_features].to_parquet(output_path)
```

### Exemplo 3: Treinar Modelo

```python
from src.pipelines.training import train, modeling

# Carregar features
X = pd.read_parquet(cm.get_table_path('features_train', 'ml'))
y = pd.read_parquet(cm.get_table_path('target_train', 'ml'))

# Treinar modelo
model, metrics = train.train_model(
    X, y,
    model_type='xgboost',
    hyperparameters={'n_estimators': 100, 'max_depth': 6}
)

# Salvar modelo
model_path = cm.get_model_path('flexo_model', version='20241123')
modeling.save_model(model, model_path)

print(f"Modelo salvo em: {model_path}")
print(f"M√©tricas: {metrics}")
```

### Exemplo 4: Fazer Predi√ß√µes

```python
from src.pipelines.inference import predict
from src.ml_artifacts import model_persistence

# Carregar modelo
model_path = cm.get_model_path('flexo_model', version='20241123')
model = model_persistence.load_model(model_path)

# Carregar dados novos
X_new = pd.read_parquet(cm.get_table_path('new_data', 'trusted'))

# Fazer predi√ß√µes
predictions = predict.batch_predict(model, X_new)

# Salvar predi√ß√µes
pred_path = cm.refined_path / 'predictions_20241123.parquet'
predictions.to_parquet(pred_path)
```

---

## üéØ Checklist de Produtiza√ß√£o

### Fase 1: C√≥digo (Conclu√≠do ‚úÖ)
- [x] Eliminar c√≥digo duplicado
- [x] Parametrizar paths
- [x] Estrutura FTI
- [x] Config centralizado
- [x] Separar POC de Produ√ß√£o

### Fase 2: Testes (Pendente ‚ö†Ô∏è)
- [ ] Testes unit√°rios (cobertura > 70%)
- [ ] Testes de integra√ß√£o
- [ ] Valida√ß√£o de schemas
- [ ] CI/CD completo

### Fase 3: Versionamento (Pendente ‚ö†Ô∏è)
- [ ] Implementar MLflow
- [ ] Model Registry
- [ ] Feature Store
- [ ] Data versioning (DVC)

### Fase 4: Monitoramento (Pendente ‚ö†Ô∏è)
- [ ] Data drift detection
- [ ] Model performance tracking
- [ ] Alertas autom√°ticos
- [ ] Dashboards de monitoramento

### Fase 5: Deploy (Pendente ‚ö†Ô∏è)
- [ ] Containeriza√ß√£o (Docker)
- [ ] API de infer√™ncia (FastAPI)
- [ ] Batch scoring jobs
- [ ] Rollback strategy

---

## üîß Migra√ß√£o de C√≥digo Antigo

### Se voc√™ tem c√≥digo usando paths antigos:

```python
# ANTIGO (quebra agora):
DATA_DIR = "/home/adami/Documentos/Projeto_IA_AMCOM/project_data_science/data/"
df = pd.read_parquet(f"{DATA_DIR}/raw/tb_pedidos.parquet")

# MIGRAR PARA:
from src.config_manager import get_config_manager
cm = get_config_manager()
df = pd.read_parquet(cm.get_table_path('tb_pedidos', 'raw'))
```

### Se voc√™ tem imports de m√≥dulos renomeados:

```python
# ANTIGO:
from src.model import model_persistence

# MIGRAR PARA:
from src.ml_artifacts import model_persistence
```

### Se voc√™ usava notebooks em src/:

```
# ANTIGO:
src/pipelines/DS/pipeline_cv_ml.ipynb

# NOVO LOCALIZA√á√ÉO:
notebooks/experiments/ds-pipelines/pipeline_cv_ml.ipynb

# RECOMENDA√á√ÉO: Extrair c√≥digo para m√≥dulos Python!
```

---

## üìä M√©tricas da Refatora√ß√£o

| M√©trica | Antes | Depois | Melhoria |
|---------|-------|--------|----------|
| Arquivos duplicados | 12 | 0 | ‚úÖ -100% |
| Hardcoded paths | 7 | 0 | ‚úÖ -100% |
| Notebooks em src/ | 8 | 0 | ‚úÖ -100% |
| Linhas de c√≥digo duplicado | ~20k | 0 | ‚úÖ -100% |
| Cobertura de testes | 0% | 0% | ‚ö†Ô∏è Pendente |
| Documenta√ß√£o (READMEs) | 1 | 17 | ‚úÖ +1600% |

---

## üö¶ Pr√≥ximos Passos Recomendados

### Curto Prazo (1-2 semanas)
1. **Implementar testes** (pytest)
2. **Validar schemas** de dados
3. **Configurar CI/CD** b√°sico

### M√©dio Prazo (1 m√™s)
4. **Implementar MLflow** para tracking
5. **Criar Feature Store** versionado
6. **Automatizar pipelines** com Airflow

### Longo Prazo (3 meses)
7. **Monitoramento** de drift
8. **API de infer√™ncia** (FastAPI)
9. **Deploy em produ√ß√£o** (Docker/K8s)

---

## üìö Refer√™ncias

- **CRISP-DM**: https://www.datascience-pm.com/crisp-dm-2/
- **CD4ML**: https://martinfowler.com/articles/cd4ml.html
- **Cookiecutter Data Science**: https://drivendata.github.io/cookiecutter-data-science/
- **MLOps**: https://ml-ops.org/

---

## üí° Dicas e Boas Pr√°ticas

### ‚úÖ Fa√ßa
- Use `config_manager` para todos os paths
- Separe POC de c√≥digo de produ√ß√£o
- Escreva testes para pipelines cr√≠ticos
- Versione modelos e features
- Documente decis√µes importantes

### ‚ùå N√£o Fa√ßa
- Hardcode paths no c√≥digo
- Misture notebooks com c√≥digo `.py` em `src/`
- Duplique c√≥digo (DRY principle)
- Commit dados sens√≠veis
- Pule etapa de testes

---

**Desenvolvido com ‚ù§Ô∏è para produtiza√ß√£o de ML**

*√öltima atualiza√ß√£o: 2024-11-23*
