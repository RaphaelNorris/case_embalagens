# ğŸ”§ Source Code (`src/`)

## PropÃ³sito
CÃ³digo-fonte modular e reutilizÃ¡vel do projeto de ciÃªncia de dados.

---

## ğŸ“ Estrutura de MÃ³dulos

```
src/
â”œâ”€â”€ ğŸ“Š dashboards/          # Dashboards interativos (Streamlit)
â”œâ”€â”€ ğŸ”Œ data/                # ConexÃµes e qualidade de dados
â”œâ”€â”€ ğŸ”¬ analysis/            # Processamento e anÃ¡lise
â”œâ”€â”€ ğŸ› ï¸ features/            # Feature engineering
â”œâ”€â”€ ğŸ¤– models/              # Treinamento e prediÃ§Ã£o ML
â”œâ”€â”€ ğŸ“ˆ viz/                 # VisualizaÃ§Ãµes (Plotly)
â”œâ”€â”€ âš™ï¸ config.py            # ConfiguraÃ§Ãµes centralizadas
â””â”€â”€ ğŸ“ logger.py            # Sistema de logging
```

---

## ğŸ—ºï¸ VisÃ£o Geral dos MÃ³dulos

### ğŸ“Š [dashboards/](./dashboards/)
**AplicaÃ§Ãµes web interativas** para visualizaÃ§Ã£o e anÃ¡lise.

**Principais arquivos:**
- `dashboard_main.py` - Dashboard multi-pÃ¡gina principal
- `dashboard_facas.py` - Dashboard especializado em facas
- `components/ui.py` - Componentes UI reutilizÃ¡veis

**Uso:**
```bash
make app-main    # Dashboard principal
make app-facas   # Dashboard de facas
```

[ğŸ“– DocumentaÃ§Ã£o completa â†’](./dashboards/README.md)

---

### ğŸ”Œ [data/](./data/)
**Gerenciamento de conexÃµes** com bancos de dados e **qualidade de dados**.

**Principais arquivos:**
- `conn_oracle.py` - ConexÃµes Oracle (Bronze/Silver/Gold)
- `conn_sql.py` - ConexÃµes SQL Server
- `data_quality.py` - ValidaÃ§Ã£o e monitoramento

**Uso:**
```python
from src.data.conn_oracle import oracle_connection

with oracle_connection('trusted') as conn:
    df = pd.read_sql("SELECT * FROM tb_clientes", conn)
```

[ğŸ“– DocumentaÃ§Ã£o completa â†’](./data/README.md)

---

### ğŸ”¬ [analysis/](./analysis/)
**Processamento e anÃ¡lise** de dados para limpeza e transformaÃ§Ã£o.

**Principais arquivos:**
- `data_processing.py` - UtilitÃ¡rios de transformaÃ§Ã£o

**FunÃ§Ãµes principais:**
- `clean_numeric_and_categorical()` - Limpeza automÃ¡tica
- `calculate_differences()` - ComparaÃ§Ã£o entre datasets
- `create_temporal_aggregation()` - AgregaÃ§Ãµes temporais
- `abc_classification()` - AnÃ¡lise de Pareto

**Uso:**
```python
from src.analysis.data_processing import clean_numeric_and_categorical

df_clean, num_cols, cat_cols = clean_numeric_and_categorical(df_raw)
```

[ğŸ“– DocumentaÃ§Ã£o completa â†’](./analysis/README.md)

---

### ğŸ› ï¸ [features/](./features/)
**Feature engineering** para modelos de Machine Learning.

**Principais arquivos:**
- `build_features.py` - CriaÃ§Ã£o de features

**FunÃ§Ãµes principais:**
- `create_temporal_features()` - Features de data/hora
- `create_production_features()` - Rolling stats e lags
- `create_stoppage_features()` - AnÃ¡lise de paradas
- `merge_temporal_production_data()` - Merge temporal

**Uso:**
```python
from src.features.build_features import create_temporal_features, create_production_features

df = create_temporal_features(df, 'dt_inicio')
df = create_production_features(df, group_cols=['cod_maquina'])
```

[ğŸ“– DocumentaÃ§Ã£o completa â†’](./features/README.md)

---

### ğŸ“ˆ [viz/](./viz/)
**VisualizaÃ§Ãµes interativas** usando Plotly.

**Principais arquivos:**
- `plots.py` - Biblioteca de grÃ¡ficos

**FunÃ§Ãµes principais:**
- `plot_time_series()` - SÃ©ries temporais
- `plot_box_by_category()` - Box plots
- `plot_pareto()` - AnÃ¡lise ABC
- `plot_histogram()` - Histogramas
- `plot_3d_scatter()` - Scatter 3D

**Uso:**
```python
from src.viz.plots import plot_time_series, plot_pareto

fig = plot_time_series(df, x_col='dt_inicio', y_col='quantidade', title='ProduÃ§Ã£o')
fig.show()
```

[ğŸ“– DocumentaÃ§Ã£o completa â†’](./viz/README.md)

---

## âš™ï¸ ConfiguraÃ§Ã£o Global

### `config.py`
ConfiguraÃ§Ãµes centralizadas usando **Pydantic**.

**Classes:**
- `OracleConfig` - ConexÃµes Oracle
- `SQLServerConfig` - ConexÃµes SQL Server
- `DataPathsConfig` - Caminhos de dados
- `MLConfig` - ConfiguraÃ§Ãµes de ML
- `AppConfig` - ConfiguraÃ§Ã£o geral

**Uso:**
```python
from src.config import get_config

config = get_config()
print(config.oracle.raw.user)
print(config.data_paths.raw)
```

**ConfiguraÃ§Ã£o via `.env`:**
```bash
# Oracle
ORACLE_RAW_USER=user
ORACLE_RAW_PASSWORD=pass
ORACLE_RAW_DSN=host:1521/service

# Paths
DATA_PATH_RAW=data/01 - raw
DATA_PATH_TRUSTED=data/02 - trusted
```

---

## ğŸ“ Logging

### `logger.py`
Sistema de logging usando **Loguru**.

**Features:**
- Logs estruturados (JSON em produÃ§Ã£o)
- RotaÃ§Ã£o automÃ¡tica de arquivos
- NÃ­veis configurÃ¡veis (DEBUG, INFO, WARNING, ERROR)

**Uso:**
```python
from src.logger import logger

logger.info("Iniciando processamento...")
logger.warning(f"Encontrados {n} valores nulos")
logger.error("Falha na conexÃ£o", exc_info=True)
```

**NÃ­veis de log:**
- `DEBUG` - Detalhes tÃ©cnicos (desenvolvimento)
- `INFO` - InformaÃ§Ãµes gerais (progresso)
- `WARNING` - Avisos (nÃ£o crÃ­tico)
- `ERROR` - Erros (falhas recuperÃ¡veis)
- `CRITICAL` - Erros crÃ­ticos (falhas fatais)

---

## ğŸ”„ Fluxo de Dados

### Pipeline TÃ­pico:

1. **ExtraÃ§Ã£o** (`data/`)
   ```python
   from src.data.conn_oracle import oracle_connection

   with oracle_connection('raw') as conn:
       df = pd.read_sql("SELECT * FROM tb_tarefcon", conn)
   ```

2. **Limpeza** (`analysis/`)
   ```python
   from src.analysis.data_processing import clean_numeric_and_categorical

   df_clean, num_cols, cat_cols = clean_numeric_and_categorical(df)
   ```

3. **Feature Engineering** (`features/`)
   ```python
   from src.features.build_features import create_temporal_features

   df_features = create_temporal_features(df_clean, 'dt_inicio')
   ```

4. **Modelagem** (`models/`)
   ```python
   from src.models.train_model import train_production_model

   model, metrics = train_production_model(X, y, model_type='xgboost')
   ```

5. **VisualizaÃ§Ã£o** (`viz/`)
   ```python
   from src.viz.plots import plot_time_series

   fig = plot_time_series(df, 'dt_inicio', 'quantidade', 'ProduÃ§Ã£o')
   ```

6. **Dashboard** (`dashboards/`)
   ```python
   streamlit run src/dashboards/dashboard_main.py
   ```

---

## ğŸ§ª Testes

```
tests/
â”œâ”€â”€ unit/              # Testes unitÃ¡rios por mÃ³dulo
â”‚   â”œâ”€â”€ test_data.py
â”‚   â”œâ”€â”€ test_features.py
â”‚   â””â”€â”€ test_catboost_fix.py
â”œâ”€â”€ integration/       # Testes de integraÃ§Ã£o
â””â”€â”€ dashboards/        # Testes de dashboards
```

**Executar testes:**
```bash
make test              # Todos os testes
pytest tests/unit/     # Apenas unitÃ¡rios
pytest -v              # Verbose
pytest -k "test_oracle"  # Filtrar por nome
```

---

## ğŸ“š ConvenÃ§Ãµes de CÃ³digo

### Imports
```python
# 1. Standard library
import os
from pathlib import Path

# 2. Third-party
import pandas as pd
import numpy as np

# 3. Local
from src.config import get_config
from src.logger import logger
```

### Docstrings (Google Style)
```python
def calculate_metric(df: pd.DataFrame, column: str) -> float:
    """Calcula mÃ©trica agregada de uma coluna.

    Args:
        df: DataFrame com os dados
        column: Nome da coluna para calcular

    Returns:
        Valor da mÃ©trica calculada

    Raises:
        KeyError: Se coluna nÃ£o existir
        ValueError: Se coluna nÃ£o for numÃ©rica
    """
    return df[column].mean()
```

### Type Hints
```python
from typing import List, Dict, Tuple, Optional

def process_data(
    df: pd.DataFrame,
    columns: List[str],
    threshold: Optional[float] = None
) -> Tuple[pd.DataFrame, Dict[str, float]]:
    ...
```

---

## ğŸ› ï¸ Ferramentas de Desenvolvimento

### Linting e FormataÃ§Ã£o
```bash
make format    # Ruff format (Black-compatible)
make lint      # Ruff linting
make type      # MyPy type checking
```

### Pre-commit Hooks
```bash
pre-commit install              # Instalar hooks
pre-commit run --all-files      # Rodar manualmente
```

**Hooks configurados:**
- âœ… Ruff (linting + formatting)
- âœ… MyPy (type checking)
- âœ… Trailing whitespace
- âœ… YAML/JSON validation
- âœ… Secrets detection

---

## ğŸš€ Quick Start

### 1. Configurar ambiente
```bash
# Instalar dependÃªncias
make install-dev

# Configurar variÃ¡veis
cp .env.example .env
# Editar .env com suas credenciais
```

### 2. Testar conexÃµes
```python
from src.data.conn_oracle import oracle_connection

with oracle_connection('trusted') as conn:
    df = pd.read_sql("SELECT COUNT(*) FROM tb_clientes", conn)
    print(f"âœ… {df.iloc[0, 0]} clientes")
```

### 3. Executar pipeline
```bash
# Extrair dados
python scripts/data_extraction/extract_oracle.py

# Processar
python scripts/analysis/process_data.py

# Treinar modelo
python scripts/ml/train_models.py

# Visualizar
streamlit run src/dashboards/dashboard_main.py
```

---

## ğŸ“– DocumentaÃ§Ã£o Adicional

- [ğŸ“Š Dashboards](./dashboards/README.md)
- [ğŸ”Œ Data](./data/README.md)
- [ğŸ”¬ Analysis](./analysis/README.md)
- [ğŸ› ï¸ Features](./features/README.md)
- [ğŸ¤– Models](./models/README.md)
- [ğŸ“ˆ Viz](./viz/README.md)

---

## ğŸ¤ Contribuindo

1. Seguir convenÃ§Ãµes de cÃ³digo
2. Adicionar type hints
3. Documentar funÃ§Ãµes (docstrings)
4. Escrever testes
5. Rodar `make format lint` antes de commitar
6. Atualizar documentaÃ§Ã£o se necessÃ¡rio

---

## â“ Troubleshooting

### Erro: `ModuleNotFoundError: No module named 'src'`
**SoluÃ§Ã£o:** Executar a partir da raiz do projeto ou adicionar ao PYTHONPATH:
```bash
export PYTHONPATH="${PYTHONPATH}:/home/user/case_embalagens/project_data_science"
```

### Erro: `Config file not found`
**SoluÃ§Ã£o:** Criar `.env` a partir do `.env.example`

### Performance lenta
**SoluÃ§Ã£o:**
- Usar caching (`@functools.lru_cache`)
- Processar em chunks
- Salvar intermediÃ¡rios em Parquet
- Usar `.query()` ao invÃ©s de indexaÃ§Ã£o booleana
