# üî¨ Analysis Module

## Prop√≥sito
Fun√ß√µes de **processamento e an√°lise de dados** para limpeza, transforma√ß√£o e an√°lise explorat√≥ria.

## M√≥dulo Principal

### `data_processing.py`
Utilit√°rios para transforma√ß√£o e an√°lise de DataFrames.

---

## Fun√ß√µes Dispon√≠veis

### üßπ `clean_numeric_and_categorical(df, numeric_threshold=0.9, inplace=False)`

Separa e limpa colunas num√©ricas e categ√≥ricas automaticamente.

**L√≥gica:**
- Colunas com > 90% valores √∫nicos ‚Üí num√©rica
- Colunas com ‚â§ 90% valores √∫nicos ‚Üí categ√≥rica
- Exclui IDs (cod_*, id_*)

**Retorno:**
- `df_clean`: DataFrame limpo
- `numeric_cols`: Lista de colunas num√©ricas
- `categorical_cols`: Lista de colunas categ√≥ricas

**Exemplo:**
```python
from src.analysis.data_processing import clean_numeric_and_categorical

df_raw = pd.read_parquet('data/01 - raw/tb_clientes.parquet')

df_clean, num_cols, cat_cols = clean_numeric_and_categorical(df_raw)

print(f"‚úÖ Num√©ricas: {num_cols}")
print(f"‚úÖ Categ√≥ricas: {cat_cols}")

# Salvar na camada trusted
df_clean.to_parquet('data/02 - trusted/tb_clientes.parquet')
```

**Transforma√ß√µes aplicadas:**

**Num√©ricas:**
- Remove outliers (IQR method)
- Converte para `float64`
- Preenche NaN com mediana

**Categ√≥ricas:**
- Remove espa√ßos extras (`str.strip()`)
- Padroniza para uppercase
- Preenche NaN com `'DESCONHECIDO'`

---

### üîç `get_common_numeric_columns(df1, df2, exclude_ids=None)`

Identifica colunas num√©ricas comuns entre dois DataFrames.

**Uso t√≠pico:** Encontrar colunas para merge ou compara√ß√£o.

**Exemplo:**
```python
from src.analysis.data_processing import get_common_numeric_columns

df_pedidos = pd.read_parquet('data/02 - trusted/tb_pedidos.parquet')
df_itens = pd.read_parquet('data/02 - trusted/tb_itens.parquet')

common_cols = get_common_numeric_columns(
    df_pedidos,
    df_itens,
    exclude_ids=['cod_pedido', 'cod_item']
)

print(f"Colunas comuns: {common_cols}")
# Exemplo: ['quantidade', 'valor_unitario']
```

---

### üìä `calculate_differences(df_left, df_right, join_key, value_columns, id_columns=None)`

Calcula diferen√ßas entre valores de dois DataFrames.

**Uso t√≠pico:** Comparar pedidos vs cat√°logo, or√ßado vs realizado.

**Retorno:**
- DataFrame com colunas `{col}_left`, `{col}_right`, `{col}_diff`, `{col}_diff_pct`

**Exemplo:**
```python
from src.analysis.data_processing import calculate_differences

# Comparar pedidos vs cat√°logo de itens
df_diff = calculate_differences(
    df_left=df_pedidos,
    df_right=df_itens,
    join_key='cod_item',
    value_columns=['quantidade', 'preco_unitario'],
    id_columns=['cod_pedido', 'cod_cliente']
)

# Resultado:
# - quantidade_left (do pedido)
# - quantidade_right (do cat√°logo)
# - quantidade_diff (left - right)
# - quantidade_diff_pct ((left - right) / right * 100)

# Filtrar itens com diferen√ßa > 10%
df_diff_significativa = df_diff[df_diff['quantidade_diff_pct'].abs() > 10]

print(f"‚ùå {len(df_diff_significativa)} itens com diferen√ßa > 10%")
df_diff_significativa.to_parquet('data/04 - refined/pedidos_itens_diff.parquet')
```

---

### üìÖ `create_temporal_aggregation(df, date_col, freq='M', agg_col=None, agg_func='count')`

Agrega dados por per√≠odo temporal.

**Frequ√™ncias dispon√≠veis:**
- `'D'` ‚Üí Di√°rio
- `'W'` ‚Üí Semanal
- `'M'` ‚Üí Mensal
- `'Q'` ‚Üí Trimestral
- `'Y'` ‚Üí Anual

**Fun√ß√µes de agrega√ß√£o:**
- `'count'`, `'sum'`, `'mean'`, `'median'`, `'min'`, `'max'`, `'std'`

**Exemplo:**
```python
from src.analysis.data_processing import create_temporal_aggregation

df_tarefcon = pd.read_parquet('data/02 - trusted/tb_tarefcon.parquet')

# Agrega√ß√£o mensal de quantidade
df_monthly = create_temporal_aggregation(
    df_tarefcon,
    date_col='dt_inicio',
    freq='M',
    agg_col='quantidade',
    agg_func='sum'
)

print(df_monthly.head())
# dt_inicio  quantidade
# 2024-01    15000
# 2024-02    18500
# 2024-03    21000

# Salvar na camada refined
df_monthly.to_parquet('data/04 - refined/producao_mensal.parquet')
```

**M√∫ltiplas agrega√ß√µes:**
```python
# Agrupar por m√™s e m√°quina com m√∫ltiplas m√©tricas
df_monthly_multi = df_tarefcon.groupby([
    pd.Grouper(key='dt_inicio', freq='M'),
    'cod_maquina'
]).agg({
    'quantidade': ['sum', 'mean', 'count'],
    'tempo_parada': 'sum'
}).reset_index()

df_monthly_multi.to_parquet('data/04 - refined/kpis_producao_mensal.parquet')
```

---

### üìà `abc_classification(values, thresholds=(80, 95))`

Classifica itens em categorias ABC (Pareto).

**Retorno:**
- DataFrame com colunas: `value`, `pct`, `cumsum_pct`, `class`

**Classes:**
- **A:** At√© 80% do total (alta import√¢ncia)
- **B:** 80-95% do total (import√¢ncia m√©dia)
- **C:** 95-100% do total (baixa import√¢ncia)

**Exemplo:**
```python
from src.analysis.data_processing import abc_classification

# Classificar clientes por quantidade
clientes_qtd = df.groupby('cod_cliente')['quantidade'].sum().sort_values(ascending=False)

df_abc = abc_classification(clientes_qtd, thresholds=(80, 95))

print(df_abc.groupby('class')['value'].agg(['count', 'sum']))
#        count      sum
# class
# A         15   800000  (15 clientes = 80% da quantidade)
# B         35   150000  (35 clientes = 15% da quantidade)
# C        150    50000  (150 clientes = 5% da quantidade)

# Salvar classifica√ß√£o
df_abc.to_parquet('data/04 - refined/abc_clientes.parquet')
```

**Visualizar:**
```python
from src.viz.plots import plot_pareto

fig = plot_pareto(clientes_qtd, title='An√°lise ABC de Clientes', cumsum_lines=[80, 95])
fig.show()
```

---

### üîó `create_pivot_table(df, index, columns, values, aggfunc='sum')`

Cria tabela din√¢mica (pivot table).

**Exemplo:**
```python
from src.analysis.data_processing import create_pivot_table

# Quantidade por m√°quina e m√™s
pivot = create_pivot_table(
    df_tarefcon,
    index='cod_maquina',
    columns='month',
    values='quantidade',
    aggfunc='sum'
)

print(pivot)
#              1      2      3      4      5
# cod_maquina
# MAQ001      1500   1800   2100   1900   2200
# MAQ002      1200   1400   1600   1500   1700

# Visualizar heatmap
from src.viz.plots import plot_heatmap
fig = px.imshow(pivot, title='Produ√ß√£o por M√°quina e M√™s', text_auto=True)
fig.show()
```

---

### üìâ `detect_outliers(df, column, method='iqr', threshold=1.5)`

Detecta outliers em coluna num√©rica.

**M√©todos:**
- `'iqr'` ‚Üí Interquartile Range (padr√£o)
- `'zscore'` ‚Üí Z-Score (desvios padr√£o)
- `'isolation_forest'` ‚Üí Isolation Forest (ML)

**Exemplo:**
```python
from src.analysis.data_processing import detect_outliers

# Detectar outliers em quantidade
outliers_mask = detect_outliers(
    df_tarefcon,
    column='quantidade',
    method='iqr',
    threshold=1.5  # 1.5 * IQR
)

df_outliers = df_tarefcon[outliers_mask]
print(f"‚ùå {len(df_outliers)} outliers detectados")

# Visualizar
import plotly.express as px
fig = px.box(df_tarefcon, y='quantidade', points='outliers')
fig.show()

# Remover outliers
df_clean = df_tarefcon[~outliers_mask]
df_clean.to_parquet('data/02 - trusted/tb_tarefcon_clean.parquet')
```

---

## Pipeline de Limpeza Completo

```python
import pandas as pd
from src.analysis.data_processing import (
    clean_numeric_and_categorical,
    detect_outliers,
    create_temporal_aggregation
)
from src.logger import logger

# ========================================
# 1. CARREGAR DADOS BRUTOS
# ========================================
logger.info("üì• Carregando dados brutos...")
df_raw = pd.read_parquet('data/01 - raw/tb_tarefcon.parquet')
logger.info(f"‚úÖ {len(df_raw)} registros carregados")

# ========================================
# 2. LIMPEZA INICIAL
# ========================================
logger.info("üßπ Limpando dados...")

# Remover duplicados
df = df_raw.drop_duplicates()
logger.info(f"‚ùå {len(df_raw) - len(df)} duplicados removidos")

# Separar e limpar num√©ricas/categ√≥ricas
df, num_cols, cat_cols = clean_numeric_and_categorical(df)
logger.info(f"‚úÖ {len(num_cols)} num√©ricas, {len(cat_cols)} categ√≥ricas")

# ========================================
# 3. DETECTAR E REMOVER OUTLIERS
# ========================================
logger.info("üîç Detectando outliers...")
outliers_mask = detect_outliers(df, 'quantidade', method='iqr')
logger.info(f"‚ùå {outliers_mask.sum()} outliers detectados")

# Op√ß√£o 1: Remover outliers
df_clean = df[~outliers_mask]

# Op√ß√£o 2: Substituir por limites (winsorizing)
# Q1 = df['quantidade'].quantile(0.25)
# Q3 = df['quantidade'].quantile(0.75)
# IQR = Q3 - Q1
# lower = Q1 - 1.5 * IQR
# upper = Q3 + 1.5 * IQR
# df['quantidade'] = df['quantidade'].clip(lower, upper)

# ========================================
# 4. VALIDAR QUALIDADE
# ========================================
from src.data.data_quality import check_data_quality

quality = check_data_quality(df_clean)
logger.info(f"üìä Missing: {quality['missing_pct']:.2f}%")
logger.info(f"üìä Duplicados: {quality['duplicates']}")

# ========================================
# 5. SALVAR NA CAMADA TRUSTED
# ========================================
logger.info("üíæ Salvando dados limpos...")
df_clean.to_parquet('data/02 - trusted/tb_tarefcon.parquet')
logger.info(f"‚úÖ {len(df_clean)} registros salvos")

# ========================================
# 6. CRIAR AGREGA√á√ïES (REFINED)
# ========================================
logger.info("üìà Criando agrega√ß√µes...")

# Agrega√ß√£o mensal
df_monthly = create_temporal_aggregation(
    df_clean,
    date_col='dt_inicio',
    freq='M',
    agg_col='quantidade',
    agg_func='sum'
)
df_monthly.to_parquet('data/04 - refined/producao_mensal.parquet')

logger.info("‚úÖ Pipeline de limpeza conclu√≠do!")
```

---

## Boas Pr√°ticas

### üßπ Limpeza
- ‚úÖ Sempre validar dados antes e depois
- ‚úÖ Documentar decis√µes (remover vs imputar)
- ‚úÖ Preservar dados brutos (01 - raw)
- ‚úÖ Versionar transforma√ß√µes (scripts/data_processing.py)

### üìä An√°lise
- ‚úÖ Come√ßar com `.info()`, `.describe()`, `.isna().sum()`
- ‚úÖ Visualizar distribui√ß√µes antes de limpar
- ‚úÖ Documentar outliers (por que removeu?)
- ‚úÖ Validar integridade referencial (foreign keys)

### ‚ö° Performance
- ‚úÖ Usar `.query()` ao inv√©s de indexa√ß√£o booleana
- ‚úÖ Processar em chunks para grandes volumes
- ‚úÖ Usar categorias para strings repetidas (`df['col'].astype('category')`)
- ‚úÖ Salvar em Parquet (comprimido, mais r√°pido)

---

## Troubleshooting

### Erro: `ValueError: could not convert string to float`
**Solu√ß√£o:** Limpar caracteres especiais antes de converter: `df['col'].str.replace(',', '').astype(float)`

### Erro: `KeyError: column not found`
**Solu√ß√£o:** Verificar se coluna existe: `if 'col' in df.columns:`

### Performance lenta em `groupby`
**Solu√ß√£o:**
- Converter colunas categ√≥ricas: `.astype('category')`
- Filtrar dados antes de agrupar
- Usar `.agg()` ao inv√©s de m√∫ltiplos `.apply()`

### Muitos outliers detectados
**Solu√ß√£o:**
- Revisar threshold (tentar 2.0 ou 3.0 ao inv√©s de 1.5)
- Usar m√©todo alternativo (zscore ao inv√©s de IQR)
- Investigar se outliers s√£o leg√≠timos (erros vs valores reais)
